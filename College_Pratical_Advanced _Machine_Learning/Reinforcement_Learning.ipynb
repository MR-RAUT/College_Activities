{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137a6a3d-5899-4238-b960-84ce21fa5ac0",
   "metadata": {},
   "source": [
    "5. Reinforcement Learning-\n",
    "a. Calculating Reward b. Discounted Reward\n",
    "c. Calculating Optimal quantities\n",
    "d. Implementing Q Learning.\n",
    "c. Setting up an Optimal Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8673291b-c378-4d65-a935-3b23a50d9d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  datadate   tic      adjcp       open       high        low  \\\n",
      "0           0  20090102  AAPL  12.964286  12.268571  13.005714  12.165714   \n",
      "1           1  20090102   AXP  19.330000  18.570000  19.520000  18.400000   \n",
      "2           2  20090102    BA  45.250000  42.800000  45.560000  42.780000   \n",
      "3           3  20090102   CAT  46.910000  44.910000  46.980000  44.710000   \n",
      "4           4  20090102  CSCO  16.960000  16.410000  17.000000  16.250000   \n",
      "\n",
      "       volume  macd    rsi        cci    adx  turbulence  \n",
      "0  26641980.0   0.0  100.0  66.666667  100.0         0.0  \n",
      "1  10955620.0   0.0  100.0  66.666667  100.0         0.0  \n",
      "2   7010171.0   0.0  100.0  66.666667  100.0         0.0  \n",
      "3   7116726.0   0.0    0.0  66.666667  100.0         0.0  \n",
      "4  40977480.0   0.0  100.0  66.666667  100.0         0.0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Load the trading dataset\n",
    "df = pd.read_csv('data/trading.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3baf2ec-e20c-4ec8-abf8-a2cf563599e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the daily return as reward\n",
    "df['reward'] = df['adjcp'].pct_change()  \n",
    "df['reward'] = df['reward'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83016273-d13c-4ba4-a225-563f6f80089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   datadate    reward  discounted_reward\n",
      "0  20090102  0.000000           0.000000\n",
      "1  20090102  0.491019           0.491019\n",
      "2  20090102  1.340921           1.782838\n",
      "3  20090102  0.036685           1.641239\n",
      "4  20090102 -0.638457           0.838659\n"
     ]
    }
   ],
   "source": [
    "# Initialize the 'discounted_reward' column as float\n",
    "df['discounted_reward'] = 0.0  # Set as float instead of integer\n",
    "\n",
    "# Calculate discounted rewards\n",
    "for t in range(len(df)):\n",
    "    if t == 0:\n",
    "        df.at[t, 'discounted_reward'] = df.at[t, 'reward']\n",
    "    else:\n",
    "        df.at[t, 'discounted_reward'] = df.at[t, 'reward'] + gamma * df.at[t - 1, 'discounted_reward']\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df[['datadate', 'reward', 'discounted_reward']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "018f8ba0-ba18-415c-a4cf-355c36cff905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   datadate    reward  optimal_quantity\n",
      "0  20090102  0.000000                -1\n",
      "1  20090102  0.491019                 1\n",
      "2  20090102  1.340921                 1\n",
      "3  20090102  0.036685                 1\n",
      "4  20090102 -0.638457                -1\n"
     ]
    }
   ],
   "source": [
    "# For demonstration, let's define a simple strategy based on rewards\n",
    "df['optimal_quantity'] = df['reward'].apply(lambda x: 1 if x > 0 else -1)  # Buy (1) if reward is positive, sell (-1) if negative\n",
    "\n",
    "# Display the DataFrame with optimal quantities\n",
    "print(df[['datadate', 'reward', 'optimal_quantity']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48f1c6d9-1b3c-4f60-a08b-aff6f169c492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table:\n",
      " [[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " ...\n",
      " [4.96354012 1.167423   1.22216428]\n",
      " [5.42494461 1.98873377 1.46851743]\n",
      " [4.33767547 5.65171285 4.19859752]]\n",
      "Optimal action for the last state: Buy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialize Q-table\n",
    "states = range(len(df))  # States could be the index of the DataFrame\n",
    "actions = [0, 1, -1]  # 0: hold, 1: buy, -1: sell\n",
    "Q = np.zeros((len(df), len(actions)))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration rate\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(100):  # Number of episodes\n",
    "    state = random.choice(states)  # Start from a random state\n",
    "    for t in range(len(df) - 1):  # Loop through time steps\n",
    "        # Choose action (Explore vs Exploit)\n",
    "        if random.uniform(0, 1) < epsilon:  # Explore\n",
    "            action_index = random.choice([0, 1, 2])  # Pick a random index for the action (0 -> hold, 1 -> buy, 2 -> sell)\n",
    "        else:  # Exploit\n",
    "            action_index = np.argmax(Q[state])  # Choose best action index from the Q-table\n",
    "\n",
    "        action = actions[action_index]  # Map action_index to actual action value (0, 1, or -1)\n",
    "\n",
    "        # Perform action and observe the reward\n",
    "        reward = df.at[t, 'reward'] if action == 1 else -df.at[t, 'reward'] if action == -1 else 0\n",
    "        \n",
    "        # Get the next state\n",
    "        next_state = min(state + 1, len(df) - 1)\n",
    "\n",
    "        # Update Q-value\n",
    "        Q[state, action_index] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action_index])\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "# Display the Q-table\n",
    "print(\"Q-Table:\\n\", Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c80d8785-2705-41ba-a061-b46b8667e795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal action for the last state: Buy\n"
     ]
    }
   ],
   "source": [
    "# Determine optimal action for the last state\n",
    "optimal_action = np.argmax(Q[-1])  # Optimal action for the last state\n",
    "action_dict = {0: 'Hold', 1: 'Buy', -1: 'Sell'}\n",
    "print(f\"Optimal action for the last state: {action_dict[optimal_action]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
